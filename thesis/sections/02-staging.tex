% Заголовки разделов и подразделов в~данном документе приводятся для примера
% и~должны быть заменены на более содержательные, отражающие суть работы.
% Можно оставить <<Введение>> и <<Заключение>>.

% \subsection{Основные понятия и определения}
% Вводятся общепринятые понятия и~обозначения со~ссылками на~литературу.

% \subsection{Обзор современного состояния проблемы}
% Излагаются известные результаты со~ссылками на~литературу.
% Необходимые для дальнейшего известные факты
% могут формулироваться как теоремы без доказательств, но~со~ссылками на~источники.

% \begin{Def}
%     Математический текст \emph{хорошо структурирован},
%     если в~нём выделены определения, теоремы, утверждения, примеры, и~т.\,д.,
%     а~неформальные рассуждения (мотивации, интерпретации)
%     выделены в~отдельные подпараграфы
%     (для этого хорошо подходит команда \verb|\paragraph{|\emph{текст}\verb|}|).
% \end{Def}

% \begin{Corollary}
%     Надо уделять большое внимание не~только формальному изложению,
%     но~и~неформальным мотивациям и~интерпретациям результатов.
%     Иначе основные идеи работы невозможно будет понять быстро.
% \end{Corollary}

% \begin{Corollary}
%     Основные идеи Вашего текста должны оставаться в~целом понятными, если читать его, пропуская все формулы.
% \end{Corollary}

% \begin{Remark}
%     Здесь показано применение окружений
%     Def, Theorem, Corollary, Remark.
% \end{Remark}

% \subsection{Формальная постановка задачи}
% К данному моменту читатель ознакомлен со всеми необходимыми понятиями
% и~подготовлен к~точным формулировкам задач, решаемых автором в~данной работе.

\subsection{Основные понятия и определения}
Обучение с подкреплением занимается задачами последовательного принятия решений. 
Данная задача в большинстве случаев формализуется в терминах Марковского Процесса Принятия Решений (МППР), определяющегося следующим образом:
\begin{Def}
    Марковский Процесс Принятия Решений (МППР) задается кортежем из четырех элементов $\mathcal{M} = \left<\mathcal{S}, \mathcal{A}, P, \mathcal{R}\right>$, где:
    \begin{itemize}
        \item $\mathcal{S}$ - набор состояний;
        \item $\mathcal{A}$ - набор действий;
        \item $P(s_{t+1}\mid s_t, a_t)$ - функция перехода состояний среды;
        \item $\mathcal{R}(s_t, a_t, s_{t+1})$ - функция вознаграждения среды;
    \end{itemize}
    \label{mdp}
\end{Def}
В работе рассматривается случай конечного МППР с вероятностной функцией перехода состояний. 
Состояние и действие предполагаются конечномерными векторами $s_t \in \mathbb{R}^{d_{s}}, a_t \in \mathbb{R}^{d_{a}}$.

Поведение агента определяется действиями, совершаемыми им в тех или иных состояниях среды. 
Агент принимает их в соответствии со \textit{стратегией} $\pi (a \mid s)$, представляющей собой функцию, ставящую в соответствие каждому состоянию среды $s_t$ действие, которое агент должен предпринять. 
В общем виде эта функция имеет вид вероятностного распределения над множеством действий.

Для полного определения задачи обучения с подкреплением также необходимо задать оптимизируемый функционал.
Вводится понятие \textit{коэффициента дисконтирования} $\gamma \in \left[0,1\right]$, отвечающего за уменьшение текущей ценности награды, которую агент может получить в будущем. Используя это понятие, формулируется следующее:
 \begin{Def}
    Отдача $R(\tau) = \sum_{t=0}^{T}\gamma^t r_t$ - дисконтированная сумма вознаграждений на определенной траектории $\tau$.
 \end{Def}
Отдача является случайной величиной, поскольку и выбор действий агентом, и функция перехода состояний среды могут иметь случайный характер в общем случае. 
Таким образом, от стратегии агента ожидается, что матожидание отдачи по всем траекториям, которые мы можем получить, будет максимальным. 
Формально это выражается следующим образом:
\begin{Def}
    Целевая функция, определяемая как $G(\pi, \mathcal{M}) = \mathbb{E}_{\tau \sim \pi}\left[R\left(\tau\right)\right] = \mathbb{E}_{\pi}\sum_{t}\gamma^t r_t$, есть математическое ожидание отдачи за эпизод, полученный при помощи использования агентом стратегии $\pi$. 
\end{Def}
Задача агента в обучении с подкреплением в общем случае - максимизировать целевую функцию $G(\pi, \mathcal{M})$ путем изменения стратегии $\pi$.
Для упрощения задачи вводится \textit{функция полезности} $V^{\pi}(s) = \mathbb{E}_{s_o=s,\tau \sim \pi}\left[R(\tau)\right] = \mathbb{E}_{s_o=s,\tau \sim \pi}\left[\sum_{t=0}^{T}\gamma^t r_t\right]$, означающая ожидаемую отдачу в эпизоде, если агент следует стратегии $\pi$ и стартует из состояния $s$. Алгоритмы, фокусирующиеся на модели среды, также пытаются аппроксимировать истинную функцию перехода состояний $P(s_{t+1} \mid s_t, a_t)$ своей собственной \textit{моделью среды} $\mathit{P}(s' \mid s, a)$. Также возможно моделирование функции награды $\mathit{R}(s', a, s)$ различного вида.
Совокупность моделей, использующихся алгоритмом, называется \textit{моделью мира}.

В постановке задачи, использующей МППР, подразумевается, что получаемые агентом состояния $s_t^{agent}$ и состояния среды $s_t^{env}$ одинаковы, то есть агент имеет доступ к истинному состоянию среды.
Однако в большинстве ситуаций, в особенности практических, получить такое состояние затруднительно либо вообще невозможно, например, когда агент получает в качестве наблюдения изображение с камеры.
Данный факт вынуждает разделить понятия наблюдений, получаемых агентом, и состояний среды, что приводит к новой обобщенной формулировке: 
\begin{Def}
    Частично Наблюдаемый Марковский Процесс Принятия Решений задается кортежем из шести элементов $\mathcal{M} = \left<\mathcal{S}, \mathcal{A}, P, \mathcal{R}, \Omega, \mathcal{O} \right>$, где:
    \begin{itemize}
        \item $\mathcal{S}$ - набор состояний;
        \item $\mathcal{A}$ - набор действий;
        \item $P(s_{t+1} \mid s_t, a_t)$ - функция перехода состояний среды;
        \item $\mathcal{R}(s_t, a_t, s_{t+1})$ - функция вознаграждения среды;
        \item $\Omega(o_t \mid s_t)$ - функция наблюдений;
        \item $\mathcal{O}$ - множество наблюдений.
    \end{itemize}
    \label{pomdp}
\end{Def}

Предполагается, что состояние $s_t$ является функцией от истории эпизода $H_t = a_0, o_1, a_1, \dots, a_{t-1}, o_t$, такой, что $s_t = f(H_t)$ сохраняет всю информацию эпизода, необходимую для максимально возможно точного предсказания $o_{t+1}$. 
Определенное таким образом состояние $s_t$ сохраняет Марковское свойство, следовательно, может использоваться для получения всех результатов для стандартного МППР. 
Поскольку функция $f$ должна быть как можно менее дорогой в вычислительном плане, её ищут в виде:
\begin{equation}
\label{eq:pomdp-model}
    s_{t+1} = p(s_t, a_t, o_{t+1})
\end{equation}
Алгоритмы модельного подхода, в основе которых лежит частично наблюдаемый МППР, аппроксимируют как функцию перехода внутренних состояний среды вида 
$\mathit{P}(s' \mid s, a)$, так и функцию вида \ref{eq:pomdp-model}.

Для измерения обобщающей способности алгоритмов удобно использовать понятие контекстного МППР \cite{CMDP}:
\begin{Def}
    Контекстный МППР задается кортежем из четырех элементов $\left<\mathcal{С}, \mathcal{S}, \mathcal{A}, \mathcal{M}\left(c\right)\right>$, где:
    \begin{itemize}
        \item $\mathcal{C}$ - множество контекстов;
        \item $\mathcal{S}$ - множество состояний;
        \item $\mathcal{A}$ - множество действий;
        \item $\mathcal{M}$ - отображение из множества контекстов $c \in \mathcal{C}$ в МППР $\mathcal{M}(c)=\left(\mathcal{S}, \mathcal{A}, p^{c}(y \mid x, a), r^{c}(x), \pi_{0}^{c}\right)$
    \end{itemize}
    \label{cmdp}
\end{Def}
Контекст является скрытой переменной, определяющей конкретную задачу в контекстном МППР. 
Каждое значение контекста определяет состояния, которые могут быть встречены в эпизоде.
Отдельные части среды могут быть параметризованы значением произвольного вектора, например, значение массы какого-либо объекта. 
В этом случае, масса может быть включена в значение контекста, либо может считаться частью стохастичности среды.

Объектно-центричный подход в обучении с подкреплением не имеет устоявшейся теоретической базы. Общей чертой является разделение состояния $s_t$ на кортеж из нескольких состояний $(s_t^1, s_t^2, \dots, s_t^n)$, каждое из которых соответствует состоянию отдельной объектной сущности. В данной работе рассмотрен именно этот случай, без дополнительного разделения пространства действий. 

Возможность комбинирования объектно-ориентированного и модельного подходов заключается в возможности факторизации функций перехода состояний. В общем случае для этого используются графовые сети, однако для более простых случаев, например, взаимодействия манипулятора и единственного объекта, можно искусственно ввести причинно-следственные связи в модель мира. Благодаря этому ожидается прирост обобщающей способности алгоритма и увеличение интерпретабельности.

\subsection{Обзор современного состояния проблемы}
Поскольку работа представляет собой применение идей объектно-центричного подхода к алгоритмам обучения с подкреплением модельного подхода, будут описаны современные подходы в обоих областях.
Особое внимание уделено описанию моделей мира в работах объектно-центричного подхода.

\subsubsection{Модельный подход в обучении с подкреплением}
Серия работ \cite{planet, dreamer, dreamerv2} является одной из наиболее заметных за последние годы в модельном подходе. 
Первой в серии статей является \cite{planet}, в которой предложена новая модель мира RSSM (Recurrent State-Space Model).
Марковские состояния моделируются при помощи апостериорного распределения следующего вида:
\begin{equation}
    q\left(s_{1:T} \mid o_{1:T}, a_{1:T}\right) = 
    \prod_{t=1}^{T} q\left(s_t \mid s_{t-1}, a_{t-1}, o_t\right)
\end{equation}
По имеющимся действиям $a_{1:T}$ и наблюдениям $o_{1:T}$ вариационный автокодировщик $q$ выводит скрытые состояния $s_{1:T}$.
Имплементация $q\left(s_t \mid s_{t-1}, a_{t-1}, o_t\right)$ выдает параметры для гауссовского распределения, случайной реализацией которого является следующее состояние $s_t$.
Используя полученный автокодировщик, авторы выводят нижнюю вариационную границу для правдоподобия наблюдений:
\begin{align}
\quad &\ln p\left(o_{1: T}, r_{1: T} \mid a_{1: T}\right) 
\notag
\\
\triangleq &\ln \int \prod_{t} p\left(s_{t} \mid s_{t-1}, a_{t-1}\right) p\left(o_{t} \mid s_{t}\right) p\left(r_{t} \mid s_{t}\right) \mathrm{d} s_{1: T} 
\notag
\\
\geq &\sum_{t=1}^{T} \Big(\mathrm{E}_{q\left(s_{t} \mid o_{\leq t}, a_{<t}\right)}\left[\ln p\left(o_{t} \mid s_{t}\right) + \ln p\left(r_{t} \mid s_{t}\right)\right] \hookleftarrow
\notag
\\
- &\mathbb{E}\left[\operatorname{KL}\left[q\left(s_{t} \mid o_{\leq t}, a_{<t}\right) \| p\left(s_{t} \mid s_{t-1}, a_{t-1}\right)\right]\right]\Big) .
\label{eq:elbo}
\end{align}

Наибольший интерес представляет анализ различных архитектур состояний и функций, при помощи которых организовано взаимодействие между ними.
Авторы находят, что реализация функции переходов состояний только при помощи вероятностных распределений не является оптимальным решением.
Из-за вероятностного характера функции переходов, запоминание информации, произошедшей некоторое количество шагов назад, не является простой задачей.
Теоретически, модель может имитировать поведение детерминированной функции, однако выучить параметры таких распределений может быть проблематичным.
С другой стороны, детерминированная функция переходов, хотя и решает проблемы вероятностной, не учитывает возможной стохастики среды.

Учитывая все вышеперечисленное, авторы приходят к выводу об уместности разделения состояния на две части, одна из которых, $h_t$, генерируется при помощи детерминированной функции, а другая, $z_t$, является параметризованным распределением:
\begin{align}
& \text { Детерминированная модель переходов: } & h_{t}=f_{\phi}\left(h_{t-1}, z_{t-1}, a_{t-1}\right) 
\notag
\\
& \text { Вывод стохастической части: } & z_{t} \sim q_{\phi}\left(z_{t} \mid h_{t}, o_{t}\right) 
\notag
\\
& \text { Генерация стохастической части: } & \hat{z}_{t} \sim p_{\phi}\left(\hat{z}_{t} \mid h_{t}\right) 
\notag
\\
& \text { Декодер изображения: } & \hat{o}_{t} \sim p_{\phi}\left(\hat{o}_{t} \mid h_{t}, z_{t}\right) 
\notag
\\
& \text { Декодер награды: } & \hat{r}_{t} \sim p_{\phi}\left(\hat{r}_{t} \mid h_{t}, z_{t}\right)
\end{align}
где функция $f_{\phi}\left(h_{t-1}, z_{t-1}, a_{t-1}\right)$ является рекуррентной нейронной сетью.
Можно рассматривать $h_t$ и $z_t$ как части состояния среды, ответственные за динамику и наблюдения соответственно, поскольку во многих задачах динамика имеет более детерминированный характер, а наблюдения обладают явно выраженной стохастикой.
Авторы подчеркивают, что при выводе состояния $z_t$ необходимо получить это состояние как реализацию случайной величины из полученного распределения, чтобы не допустить детерминированного прохода информации от наблюдения $o_t$ к реконструкции $\hat{o}_t$.
Эксперименты демонстрируют, что и стохастическая часть состояния $z_t$, и детерминированная часть $h_t$ важны для успешного решения задач, в частности, без $z_t$ процесс обучения модели не приносит результатов.

В следующей статье \cite{dreamer} авторы предлагают один из сильнейших современных модельных алгоритмов, Dreamer, в основе которого лежит предложенная в прошлой статье модель мира RSSM.
Главное наблюдение, легшее в основу работы, состоит в возможности проведения эффективной в вычислительном плане тренировки стратегии агента на траекториях из скрытых марковских состояний $s_t$, полученных исключительно из модели мира без взаимодействия со средой.
Процесс обучения на сгенерированных при помощи модели переходов состояний траекториях авторы называют обучением в ``воображении'' (imagination).
Тренировка всех частей агента происходит следующим образом:
\begin{itemize}
    \item Взаимодействие со средой при использовании стратегии $\pi\left(a_t \mid s_t\right)$
    \item Тренировка модели мира на траекториях, полученных из буфера опыта $\mathcal{D}$
    \item Тренировка стратегии в ``воображении'' модели
\end{itemize}
При генерировании траекторий в пространстве скрытых марковских состояний, алгоритм не может получить никаких наблюдений, поэтому использует только модель переходов скрытых состояний $p\left(s_t \mid s_{t-1}, a_{t-1}\right)$ и стратегию $\pi\left(a_t \mid s_t\right)$.
Модель переходов скрытых состояний обучается на этапе обучения модели мира как целого.
Модель мира обучается, следуя принципам статей \cite{VIB,DeepVIB}, макисимизируя вариационную нижнюю оценку взаимной информации:
\begin{equation}
    \max \mathrm{I}\left(s_{1: T} ;\left(o_{1: T}, r_{1: T}\right) \mid a_{1: T}\right)-\beta \mathrm{I}\left(s_{1: T}, i_{1: T} \mid a_{1: T}\right),
\end{equation}
где $\beta$ - действительный гиперпараметр, а $i_t$ - индексы буфера данных, опрееделяющие наблюдения следующим образом: $p\left(o_{t} \mid i_{t}\right) \doteq \delta\left(o_{t}-\bar{o}_{t}\right)$.
Функционал, максимизируемый в процессе тренировки моделью мира, состоит из нескольких частей и выглядит следующим образом:
\begin{align}
&\mathcal{J}_{\mathrm{REC}} \doteq \mathrm{E}_{p}\left(\sum_{t}\left(\mathcal{J}_{\mathrm{O}}^{t}+\mathcal{J}_{\mathrm{R}}^{t}+\mathcal{J}_{\mathrm{D}}^{t}\right)\right)+\text { const } \quad \mathcal{J}_{\mathrm{O}}^{t} \doteq \ln q\left(o_{t} \mid s_{t}\right) \\
&\mathcal{J}_{\mathrm{R}}^{t} \doteq \ln q\left(r_{t} \mid s_{t}\right) \quad \mathcal{J}_{\mathrm{D}}^{t} \doteq-\beta \mathrm{KL}\left(p\left(s_{t} \mid s_{t-1}, a_{t-1}, o_{t}\right) \| q\left(s_{t} \mid s_{t-1}, a_{t-1}\right)\right)
\label{eq:elbo_loss}
\end{align}
Он представляет собой видоизмененную версию нижней вариационной границы для наблюдения и наград из RSSM~\ref{eq:elbo}.
Таким образом, на этапе тренировки модели мира, алгоритм получает наблюдения и награды и учится предсказывать их без использования изображений. Выученные таким способом модели используются для эффективной тренировки стратегии в ``воображении''.
Низкие вычислительные затраты на такую тренировку обеспечиваются низкой размерностью марковских состояний $s_t$ по сравнению с наблюдениями $o_t$.

Поскольку при переходе к скрытым марковским состояниям $s_t$, мы переходим к обычному МППР, для тренировки стратегии в воображении можно применять традиционные алгоритмы обучения с подкреплением. В работе использован подход актора-критика~\cite{Sutton}, то есть происходит параллельное обучение следующих моделей:
\begin{align}
    & \text { Актор: } &
    a_{\tau} \sim \pi_{\phi}\left(a_{\tau} \mid s_{\tau}\right)
    \notag
    \\
    & \text { Критик: } &
    v_{\psi}\left(s_{\tau}\right) \approx \mathrm{E}_{q\left(\cdot \mid s_{\tau}\right)}\left(\sum_{\tau=t}^{t+H} \gamma^{\tau-t} r_{\tau}\right)
\end{align}

В последующей статье \cite{dreamerv2}, предложена улучшенная версия алгоритма, содержащая в качестве $s_t$ не Гауссово распределение, а категориальное.
Авторы аргументируют выбор такого вида распределения более простой оптимизацией, возможностью более точного совпадения генеративного и апостериорного распределений и возможными особенностями среды Atari.
Также предложена техника балансирования дивергенции Кульбака-Лейблера (KL balancing), позволяющая в процессе тренировки модели мира оптимизировать генеративное $p_{\phi}\left(\hat{z}_{t} \mid h_{t}\right)$ и апостериорное $q_{\phi}\left(z_{t} \mid h_{t}, o_{t}\right) $ распределения с разной эффективностью.
Данное изменение мотивировано тем, что обучение генеративного распределения хорошего качества может быть достаточно долгим процессом, в течение которого априорное распределение будет оптимизироваться в сторону совпадения с генеративным распределением плохого качества.
Следуя этой логике, авторы предлагают увеличить шаг обучения относительно генеративного распределения.
Также предложены изменения в модели актора-критика, не представляющие большого интереса для данной работы.

\subsubsection{Алгоритмы объектно-центричного подхода}
В ряде работ в области поиска представления предложена идея структурировать переменную, отвечающую за представление изображения. 
Более формально, следуя идеям, предложенным в \cite{vae}, рассматривается датасет $\mathbf{X}=\{\mathbf{x}^{\left(i \right)}\}_{i=1}^{N}$, состоящий из $N$ независимых и одинаково распределенных сэмплов случайной величины $\mathbf{x}$. 
Предполагается, что также существует случайная величина $\mathbf{z}$, и данные сгенерированы, используя распределения $p\left(\mathbf{z}\right)$ и $p\left(\mathbf{x} \mid \mathbf{z} \right)$. 
Реализации $\mathbf{z}_i$ называются скрытыми переменными, и представляют собой сжатые представления изображений $\mathbf{x}_i$. 
В \cite{vae} предложены способы эффективной оценки некоторых распределений в данной задаче, используя вероятностную аппроксимацию апостериорного распределения $q_{\psi}\left(\mathbf{z} \mid \mathbf{x} \right)$.
Таким образом, переходя к рассмотрению объектно-центричных подходов к решению данной задачи, их отличительной чертой является введение структуры в случайную величину $\mathbf{z}$. 
Наиболее часто встречаются подходы, где $\mathbf{z}$ является кортежем из величин $\mathbf{z}_i$, каждая из которых соответствует за реализацию отдельного объекта на изображении. 
Процесс генерации $\mathbf{x}$ по $\mathbf{z}$ может отличаться.

В работе \cite{IODINE} предполагается независимость $\mathbf{z}_k \in \mathbb{R}^M$, при этом каждая из $\mathbf{z}_k$ делает вклад в изображение $\mathbf{x} \in \mathbb{R}^D$. 
Количество объектных скрытых переменных $K$ является гиперпараметром модели, представляющим верхнюю границу количества отдельных объектов на изображении. 
Генеративный механизм реализован в виде смеси гауссиан: 
\begin{equation}
    p\left(\mathbf{x} | \mathbf{z}\right) = \prod_{i=1}^{D} \sum_{k=1}^{K} m_{ik} \mathcal{N}\left(x_i; \mu_{ik}, \sigma^2 \right)
\end{equation}
Каждый пиксель $x_i$, таким образом, является вероятностной смесью различных объектных компонент $\mu_{ik}$ с весами $m_{ik}, \sum_{k=1}^{K} m_{ik} = 1, \forall i = \left[1 \dots D\right]$. 
Маска $\mu_{ik}$ и среднее значение пикселя $m_{ik}$ являются выходами декодера, принимающего на вход скрытую переменную $\mathbf{z_i}$. 
Распределение $q(\mathbf{z} | \mathbf{x})$ является нормальным c параметрами $\mathbf{\lambda} = \left(\mathbf{\mu}_{\mathbf{z}}, \mathbf{\sigma}_{\mathbf{z}}\right)$. Алгоритм обучается путем минимизации нижней вариацинной оценки обоснованности.
Несмотря на достаточно большую долю вычислений, способных проводится параллельно, параметры распределения скрытой переменной обновляются при помощи механизма итеративного вывода (iterative inference), что делает алгоритм достаточно медленным.

В работе \cite{SlotAtt} представлен иной способ выделения объектов на  изображении. 
В статье отказываются от подробного моделирования генеративного процесса изображения, предлагая модуль Slot-Attention, представляющий собой применение механизма ``внимания'' к признакам, полученным из свёрточной сети и случайно инициализированным объектным ``слотам''. 
Полученные векторы подаются в рекуррентную нейронную сеть, обновляющую ``слоты''. 
Для вывода проводят $\sim 10$ итераций этого процесса.
Полученные вектора, по сути являющиеся скрытыми объектными переменными $\mathbf{z}_k$, используются для декодирования. 
Архитектура декодера аналогична используемой в \cite{IODINE}. 
Алгоритм может быть адаптирован для широкого спектра задач, и для самой базовой, детектирования объектов на изображении, обучающий сигнал предоставляется исключительно ошибкой предсказания изображения. 
Принципиальное отличие упомянутых алгоритмов состоит в скорости работы: в \cite{IODINE}, каждый шаг итеративного вывода сопряжен с вычислительно дорогой операцией предсказания изображения и вычисления ошибки в пространстве изображений, тогда как в \cite{SlotAtt} все итерации проводятся исключительно в пространстве скрытых переменных, делая алгоритм значительно быстрее.

Введение дополнительной семантической структуры в изображение путем факторизации скрытой переменной позволяет авторам не только добиться улучшения различных показателей качества, но и производит качественное распутывание (``disentanglement'') представления изображения. 
Это качество проявляется в интепретируемости отдельных компонент представления, и в объектно-центричных методах, где объектные компоненты отвечают за различные объекты, данное свойство явно проявляется.
Однако необходимо подчеркнуть, что в методах, принимающих на вход единственное изображение, возможность выделить объект зависит преимущественно от датасета и полученные репрезентации по сути являются просто сегментациями, без возможности учета какой-либо причинности и в целом взаимосвязей между объектами.
Для возможности учета таких факторов необходимо присутствие как минимум временного контекста.
Его добавление приводит к постановке задачи предсказания видео, в котором также представлены алгоритма объектно-центричного подхода, например, \cite{oc_video_1, oc_video_2}.

Дальнейший шаг в сторону обучения с подкреплением происходит путем добавления в задачу условных факторов, то есть действий.
Промежуточную ступень между работами в области поиска представления и обучения с подкреплением представляют работы, рассматривающие задачу моделирования динамики среды.
В отличие от обучения с подкреплением, в них отсутствует понятие награды, и от предложенных алгоритмов требуется только выучить модель среды.
Присутствие нескольких объектных сущностей на изображении оставляет свободу в постановке задачи исследователям. 
В частности, сущности могут быть рассмотрены как части агента, взаимодействующие друг с другом и имеющие разделенные наборы действий. Подобная постановка задачи рассмотрена в \cite{cswm} и формализуется при помощи введения факторизованных пространств скрытых состояний и действий следующим образом:
\begin{align}
    \mathcal{Z} &
        = \mathcal{Z}_1 \times \mathcal{Z}_2 \times 
                    \dots \times \mathcal{Z}_K
                    - \text{пространство скрытых состояний;}
        \notag
    \\
    \mathcal{A} &
        = \mathcal{A}_1 \times \mathcal{A}_2 \times 
                    \dots \times \mathcal{A}_K
                    - \text{пространство действий;}
        \notag
\end{align}

Для вывода объектных скрытых переменных из изображения, используется более простая по сравнению с уже описанными подходами модель, состоящая из двух частей: выделитель объектов $E_{ext}$ и объектный энкодер $E_{enc}$. 
На первом этапе изображение подается в сверточную сеть с $K$ фильтрами на последнем слое, которые интерпретируются как объектные маски $m^k$. 
Далее, полученные выходы $m^k$ кодируются полносвязной нейронной сетью $E_{enc}$, имеющей общие веса для всех $m^k$. 
Данный процесс описывается следующим образом:
\begin{align}
    \mathbf{z}^k & = E_{enc}\left(m^k\right), \quad m^k = \left[E_{ext}\left(\mathbf{x}\right)\right]_k 
    \notag
\end{align}

Взаимодействие между различными объектами и действиями, совершенными над ними, моделируется при помощи графовой нейронной сети (GNN), использующей в качестве связей между вершинами сообщения.
Минимизируемый функционал представлен в виде адаптированной для объектной абстракции функции потерь из статьи \cite{hinge_loss}.

В работе \cite{OP3} авторы предполагают, что, хотя объекты необходимо моделировать как отдельные сущности, главный интерес представляют общие ``правила'' среды, например, законы физики, одинаково справедливые для всех объектов, присутствующих в среде. 
Моделирование подобных правил позволяет алгоритму проще адаптироваться к новым конфигурациям задач, поскольку законы физики продолжают выполняться для любого количества и комбинации объектов.
В отличие от \cite{cswm}, пространство действий не является факторизованным.
Генеративная модель наблюдений выглядит следующим образом:
\begin{equation}
    p\left(X^{(0: T)}, Z_{1: K}^{(0: T)} \mid a^{(0: T-1)}\right)=p\left(Z_{1: K}^{(0)}\right) \prod_{t=1}^{T} p\left(Z_{1: K}^{(t)} \mid Z_{1: K}^{(t-1)}, a^{(t-1)}\right) \prod_{t=0}^{T} p\left(X^{(t)} \mid Z_{1: K}^{(t)}\right)
\end{equation}
Модель мира $P(s_{t+1} \mid s_t, a_t)$ декомпозирует сложную модель взаимодействия сущностей и действия на следующие составляющие:
\begin{itemize}
    \item эффект действия $a_t$ на каждую сущность
    \item взаимодействие сущностей между собой
\end{itemize}
Важной особенностью данной модели является использование для всех сущностей и пар сущностей одних и тех же функций, за счет чего авторы добиваются полной симметричности моделирования отношений объектов.
Тогда как подобный подход позволяет добиться обобщения в комбинаторных задачах, в средах, подразумевающих несимметричность отношений объектов между собой, подобное моделирование может быть проблематичным. 
Например, моделирование взаимодействия достаточно тяжелого робота-агента и легкого кубика подразумевает почти полное пренебрежение влияния кубика на динамику робота. 
В средах, с сильно разнящимися по своим физическим свойствам объектами, выучить подобные законы взаимодействия может быть проще моделировать по отдельности.

Объектно-центричные алгоритмы обучения с подкреплением достаточно редки.
Одним из них является ROLL \cite{ROLL}, алгоритм, решающий задачу обусловленного задачей (goal-conditioned) обучения с подкреплением, то есть алгоритм вместе с обычным набором наблюдений получает также изображение среды $\mathit{I}_g$, которое необходимо достичь. В отличие от предлагаемого в работе подхода, ROLL не требует истинной сегментации и способен самостоятельно сегментировать изображения. В алгоритме применяются два отдельных VAE: для сегментированных объектов и для изначального изображения. Также, в алгоритме применяется специальный формат тренировки для большей устойчивости к перекрытию объектами и манипулятором друг друга. Явно сформулированной модели мира в алгоритме не присутствует.
Таким образом, ROLL в некотором роде дополняет предложенный в данной работе алгоритм, предлагая способы сегментации и борьбы с перекрытиями объектов, что делает его ценным для изучения в дальнейшей работе.

Еще одним примером объектно-центричных алгоритмов является GATSBI \cite{GATSBI}. Аналогично предыдущей работе, алгоритму не требуется доступа к истинным сегментациям и манипулятор, задний фон и объекты моделируются по-разному. Также, в отличие от предыдущей работы, взаимодействие между объектами моделируется при помощи графовой нейронной сети. Сегментируются различные объекты также разными способами, более подходящими для свойств конкретного типа объектов. Тренировка производится стандартным образом для моделей, работающих в постановке частично наблюдаемого МППР - при помощи максимизации нижней вариационной оценки правдоподобия. Тогда как подход, предложенный в работе демонстрирует хорошие результаты в средах с большим количеством различных простых объектов, ему может не хватить экспрессивности в случае сложного взаимодействия между единственным манипулятором и объектом со сложной динамикой.

\subsection{Формальная постановка задачи}
Ставится задача построения модели мира, обладающей структурированным скрытым состоянием и разделяющей модели переходов состояний различных объектов в среде с целью увеличения обобщающих способностей базового алгоритма при сохранении прочих показателей качества.

В среде предполагаются выполненными следующие условия задачи визуального роботизированного контроля:
\begin{itemize}
    \item В среде присутствуют манипулятор и объект, с которым манипулятор может взаимодействовать
    \item Функция награды зависит и от манипулятора, и от объекта
    \item В качестве наблюдений алгоритм получает трехмерные изображения среды
    \item Агент управляет при помощи действий только манипулятором
\end{itemize}
Также предполагаются выполненными следующие дополнительные предположения:
\begin{itemize}
    \item Имеется доступ к истинным маскам объекта и манипулятора на изображениях, получаемых из среды
    \item Имеется доступ к истинному вектору параметров конкретной задачи, называемому контекстом задачи $c$.
\end{itemize}
Задача моделируется в виде частично наблюдаемого МППР \ref{pomdp}, однако нотация заимствует некоторые понятия из постановки задачи контекстного МППР \ref{cmdp}, поскольку они являются легко адаптируемыми для рассматриваемого случая.

В качестве базового алгоритма используется Dreamer \cite{dreamer}.
Для справедливого сравнения предлагаемых моделей с базовым алгоритмом предполагается возможным использование преимуществ, приобретаемых благодаря факторизации модели мира, однако изменения модели за рамками вышеупомянутых, как например использование сильно отличающегося способа тренировки стратегии (например, изменения алгоритма актора-критика, не связанные с объектной факторизацией), считаются недопустимыми.

Для сравнения эффективности обучения алгоритма и его обобщающих способностей предлагается использовать недисконтированную усредненную по эпизодам награду, полученную за эпизод:
\begin{equation}
    \mathcal{R} = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{T} r_t\right]
\end{equation}
График значений математического ожидания по траекториям задач из тренировочного распределения служит оценкой качества обучения алгоритма.
График, полученный на тестовых данных, служит оценкой обобщающей способности алгоритма.
Важно подчеркнуть, что алгоритм обучается только на тренировочных задачах, во время оценки качества работы на тестовых алгоритм не выполняет шагов оптимизации.
Оценка графика проводится путем сравнения значений показателя качества в конце обучения.