Типичные задачи визуального контроля состоят в том, чтобы совершить какое-либо действие над объектом при помощи манипулятора.
Поскольку в формулировке задачи обучения с подкреплением при помощи МППР не присутствует в явном виде понятия объектов и сущностей, модели мира агентов модельного подхода в подавляющем большинстве случаев не учитывают структуры среды в алгоритме.
Типичным примером такой модели среды служит модель среды алгоритма Dreamer \cite{dreamer}, использованного в качестве базового алгоритма.

Тогда как в общем случае моделирование взаимодействий разумно производить, не выделяя заранее роли тех или иных сущностей, можно предположить, что в среде присутствует только манипулятор и объект (за исключением, возможно, жестко зафиксированных объектов, выполняющих роль обстановки).
В этом случае становится возможным моделирование причинных отношений агента и объекта в явном виде при помощи структуры нейронной сети.
Таким образом, в состоянии среды $s_t$ выделяется две компоненты, $s^r_t$ и $s^o_t$, соответствующие состоянию робота и состоянию объекта в момент времени $t$.
Также предположим, что в среде имеется возможность получения истинных наблюдений манипулятора и объекта, то есть в качестве наблюдения выступает пара $\left(o^r_t, o^o_t\right)$.
В дальнейшем предполагается, что в качестве наблюдений выступают сегментированные изображения среды, однако это не является строгим требованием.

Принципиальная разница между моделью переходов состояний $s^r_t$ и $s^o_t$ состоит в том, какое влияние на них оказывают действия, принимаемые агентом.
В конкретном рассматриваемом случае визуального роботизированного контроля уместно предположить, что действия агента изменяют непосредственно \textit{только} состояние манипулятора, а на состояние объекта влияют лишь косвенно.
Таким образом, мы приходим к заключению, что исходная модель функции переходов состояний может остаться неизменной для части манипулятора:
\begin{equation}
    q\left(s_{t} \mid s_{t-1}, a_{t-1}\right) 
\end{equation}

\begin{figure}[t]%{r}{0.35\textwidth}
    \centering
    % \vspace{-80pt}
    \scalebox{0.7}{
        \input{schemes/wm-comparison/dreamer}
    }
    \quad
    \scalebox{0.7}{
        \input{schemes/wm-comparison/factored_wm}
    }
    \caption{Сравнение структур моделей переходов состояний алгоритма Dreamer \cite{dreamer} и предлагаемой объектно-центричной модели мира}
    \label{fig:wm_comparison}
\end{figure}

Воздействие манипулятора на объект может быть смоделировано разными способами. Функция переходов состояний объекта должна быть обусловлена на переменную, содержащую информацию о состоянии манипулятора, поскольку предполагается, что именно манипулятор заставляет объект изменять свое состояние.
Представляются возможными два варианта:
\begin{itemize}
    \item Состояние $s^o_t$ обусловлено на состояние $s^r_t$ (прямого влияния)
    \item Состояние $s^o_t$ обусловлено на вектор $u_t$, отражающий ``влияние'' манипулятора на объекта (косвенного влияния)
\end{itemize}
Модели первого класса содержат меньшее количество скрытых переменных, упрощая генеративную модель, однако также обладают меньшим потенциалом для улучшения обобщающей способности алгоритма, так как состояние $s^r_t$ может содержать нерелевантную информацию для предсказания состояния $s^o_t$.
Сравнение моделей мира базового алгоритма и предлагаемых моделей без спецификации того, каким образом манипулятор и объект взаимодействуют друг с другом, приведено на рисунке \ref{fig:wm_comparison}.

В работе рассмотрены возможные архитектуры из обоих семейств.

\subsection{Семейство моделей прямого влияния}
Простейшей моделью перехода состояний в этом семействе является:
\begin{align}
    & \text{модель объектной динамики: } \quad & p&(s^o_t \mid s^o_{t-1}, s^r_t)
    \notag
    \\
    & \text{модель динамики робота: } \quad & p&(s^r_t \mid s^r_{t-1}, a_t)
    \notag
    \\
    & \text{модель репрезентации объекта: } \quad & q&(s^o_t \mid s^o_{t-1}, s^r_{t}, o^{o}_{t})
    \notag
    \\
    & \text{модель репрезентации робота: } \quad & q&(s^r_t \mid s^r_{t-1}, a_{t-1}, o^{r}_{t})
    \label{eq:instant}
\end{align}
Динамика робота и объекта в явном виде разделена на две части, при этом влияние объекта на робота вообще не учитывается.
Влияние же робота на объект выражено исключительно через $s^r_t$, что позволяет трактовать $s^r_t$ как абстрактное действие робота по отношению к объекту.
Подобное понятие весьма условно, поскольку в явном виде контроля над этими действиями не имеется.

Допущение незначительности влияния объекта на агента ограниченно выполняется для многих задач визуального контроля, однако в общем случае оно не является справедливым.
Даже в простейших примерах, симулирующих условия реального мира, обязано выполняться ограничение, не позволяющее манипулятору и объекту пересекаться друг с другом.
Для преодоления этих ограничений можно обусловить модель динамики робота также и на состояние объекта, однако при этом возникают проблемы с тренировкой, поскольку возникают циклы при попытке оценить апостериорную вероятность.

Следует упомянуть, что подобная генеративная модель не является единственной возможной в этом семействе.
Можно также предположить, что обновленное состояние $s^o_t$ зависит не от обновленного состояние робота $s^r_t$, а, скорее, от его предыдущего состояния $s^r_{t-1}$, что производит модель мира следующего вида:
\begin{align}
    & \text{модель объектной динамики: } \quad & p&(s^o_t \mid s^o_{t-1}, s^r_{t-1})
    \notag
    \\
    & \text{модель динамики робота: } \quad & p&(s^r_t \mid s^r_{t-1}, a_t)
    \notag
    \\
    & \text{модель репрезентации объекта: } \quad & q&(s^o_t \mid s^o_{t-1}, s^r_{t-1}, o^{o}_{t})
    \notag
    \\
    & \text{модель репрезентации робота: } \quad & q&(s^r_t \mid s^r_{t-1}, a_{t-1}, o^{r}_{t})
    \label{eq:reactive}
\end{align}
Отличие моделей может проявляться в уменьшенной ошибке, вызванной предположение о слабой обратной связи объекта и робота.
Пусть в состоянии манипулятора $s^r_{t-1}$ выполнялось данное предположение, но уже в следующем момент времени $t$ оно не является справедливым.
Состояние $s^r_t$ будет получено с ошибкой, поскольку оно не обусловлено на $s^o$.
В таком случае, в модели \ref{eq:instant} ошибка будет накапливаться, поскольку обновленное состояние $s^o_t$ обусловлено на состояние $s^r_t$, содержащее ошибку из-за не выполненного предположения в момент $t$.
В модели \ref{eq:reactive} ошибка проявится в меньшем виде, поскольку в момент $s^r_{t-1}$ предположение еще выполнено.

\subsection{Семейство моделей косвенного влияния}
Будем называть переменную, на которую обусловлено состояние объекта $s^o_t$, вектором влияния $u_t$.
Если в семействе моделей прямого влияния предполагалось, что влияние манипулятора на объект представлено в виде состояния манипулятора $u_t = s^r_t$, то в семействе моделей косвенного влияния вектор влияния $u_t$ отделен от состояния робота.
Поскольку вектор $u_t$ представляет влияние манипулятора, то логично обусловить его на состояние $s^r_t$.
Собирая вместе, получаем следующую генеративную модель:
\begin{align}
    & \text{модель объектной динамики: } \quad & p&(s^o_t \mid s^o_{t-1}, u_t)
    \notag
    \\
    & \text{модель влияния: } \quad & p&(u_t \mid s^r_{t})
    \notag
    \\
    & \text{модель динамики робота: } \quad & p&(s^r_t \mid s^r_{t-1}, a_t)
    \label{eq:u_prior}
\end{align}

В зависимости от того, на что обусловлены априорное и апостериорное распределения вектора влияния, значение влияния в модели меняется.
Представляются удобно трактуемыми и практически реализуемыми четыре варианта, представленные на Рисунке \ref{fig:inf_schemes}.

Первый вариант содержит следующие апостериорные распределения:
\begin{align}
    & \text{вывод объектного состояния: } \quad & q&(s^o_t \mid s^o_{t-1}, o^o_t)
    \notag
    \\
    & \text{вывод вектора влияния: } \quad & q&(u_t \mid s^o_t)
    \notag
    \\
    & \text{вывод состояния робота: } \quad & q&(s^r_t \mid s^r_{t-1}, a_t, u_t, o^r_t)
    \label{eq:cema-td}
\end{align}
В этом случае генеративный процесс и процесс вывода четко противопоставлены друг другу, протекая в обратных направлениях.
Тогда как данный вариант выглядит достаточно разумно, у него присутствует ряд проблем:
\begin{itemize}
    \item Апостериорное распределение для вектора влияния $u_t$ плохо сопоставимо с генеративным. 
    При проходе алгоритма снизу-вверх, $u_t$ обусловлен только на состояние робота $s^r_t$, при этом состояние объекта может быть любым. 
    Получив наблюдение изменившегося состояния объекта, попытки вывести, каким образом на объект повлиял манипулятор, выглядят странно, так как это подразумевало бы, что либо в $s^o_t$ содержится информация о манипуляторе, либо в $s^r_t$ содержится информация об объекте, чего хотелось бы избежать.
    \item Апостериорное состояние манипулятор $s^r_t$ обусловлено на вектор влияния $u_t$. Данный факт сложно проинтерпретировать, также, учитывая тот факт, что $u_t$ обусловлен на $s^o_t$, в состояние робота протекала бы информация от объекта, чего также хотелось бы избежать.
\end{itemize}

Второй вариант содержит процесс вывода, одинаково направленный с процессом генерации, формально записывающийся следующим образом:
\begin{align}
    & \text{вывод состояния робота: } \quad & q&(s^r_t \mid s^r_{t-1}, a_t, o^r_t)
    \notag
    \\
    & \text{вывод вектора влияния: } \quad & q&(u_t \mid s^r_t)
    \notag
    \\
    & \text{вывод объектного состояния: } \quad & q&(s^o_t \mid s^o_{t-1}, u_t, o^o_t)
    \label{eq:inf_2}
\end{align}
Тогда как такая архитектура выглядит странной для вывода вектора влияния, она может быть уместной для объектного состояния, поскольку в данном случае модели вывода и генерации объектной части представляют собой полную аналогию соответствующим моделям базового алгоритма.
Подобное соответствие допускает трактовку $u_t$ как уже упомянутого абстрактного действия для объектной части алгоритма.

В третьем варианте представлен фактор влияния, не обусловленный ничем на стадии вывода:
\begin{align}
    & \text{вывод состояния робота: } \quad & q&(s^r_t \mid s^r_{t-1}, a_t, u_t, o^r_t)
    \notag
    \\
    & \text{вывод объектного состояния: } \quad & q&(s^o_t \mid s^o_{t-1}, u_t, o^o_t)
    \label{eq:inf_3}
\end{align}
Вектор влияния полностью теряет смысл, вложенный в него изначально.

В последнем варианте апостериорное распределение вектора влияния обусловлено и на объектное состояние, и на состояние манипулятора:
\begin{align}
    & \text{вывод объектного состояния: } \quad & q&(s^o_t \mid s^o_{t-1}, o^o_t)
    \notag
    \\
    & \text{вывод состояния робота: } \quad & q&(s^r_t \mid s^r_{t-1}, a_t, o^r_t)
    \notag
    \\
    & \text{вывод вектора влияния: } \quad & q&(u_t \mid s^r_t, s^o_t)
    \label{eq:cema-infl}
\end{align}
Такой выбор апостериорного распределения мотивирует $u_t$ содержать информацию, полезную для предсказания состояния объекта $s^o_t$, при этом используя информацию, содержащуюся в $s^r_t$.
Таким образом, решается аналог обратной задачи динамики: по получившемуся состоянию $s^o_t$ и воздействовавшему на него состоянию $s^r_t$ мы хотим получить воздействие $u_t$.

Тогда как первый и четвертый варианты определения апостериорных распределений выглядят наиболее подходящими для использования в алгоритмах, в них присутствует недостаток, связанный с отсутствием информации об объекте в генеративном распределении $p\left(u_t \mid s^r_t \right)$.
В зависимости от того, каким образом решается эта проблема, значение вектора влияния также меняется.

\begin{figure}[t]%{r}{0.35\textwidth}
    \centering
    % \vspace{-80pt}
    \scalebox{0.75}{
        \input{schemes/inference/inference_1}
    }
    \scalebox{0.75}{
        \input{schemes/inference/inference_2}
    }
    \scalebox{0.75}{
        \input{schemes/inference/inference_3}
    }
    \scalebox{0.75}{
        \input{schemes/inference/inference_4}
    }
    \caption{Возможные виды зависимостей апостериорных распределений в модели неявного влияния}
    \label{fig:inf_schemes}
\end{figure}

\subsubsection{Однозадачный вектор влияния}
Проблема в отсутствии информации об объекте появляется из-за разницы генеративного и апостериорного распределений для вектора влияния.
Для того, чтобы генеративное распределение не слишком сильно отличалось от апостериорного, вектору влияния необходим дополнительный источник информации относительно того, где может находиться объект.
То есть, вектор влияния должен зависеть не только от состояния манипулятора, но и от состояния объекта, в той или иной мере, чтобы более точно моделировать взаимодействие в генеративном процессе.

В некоторых средах, в частности в средах с малым количеством степеней свободы объекта, контекст задачи $c \in \mathcal{C}$ может достаточно точно определять положение объекта в тот или иной момент времени.
В предположении доступа к истинным значениям этих параметров для каждой задачи в такой среде, можно обусловить $u_t$ на $c$, получая в результате следующее генеративное распределение для вектора влияния:
\begin{equation}
    p\left(u_t \mid s^r_t, c\right)
\end{equation}
Получаемый вектор влияния является обусловленным на контекст задачи, что дает возможность генерировать более точные траектории для каждой конкретной задачи при помощи модели среды.
Введение такого распределения также мотивирует апостериорное распределение любого вида из упомянутых ранее выделять информацию из наблюдений, соответствующую отдельным параметрам задач.
Также введение подобного внешнего фактора позволяет не вводить информации об объекте в состояние манипулятора.
Получается архитектура, в которой модель перехода состояний манипулятора не учитывает информацию об объекте, но при этом функция перехода состояний объекта зависит от влияния манипулятора на объект в конкретной задаче.
Для усиления обучающего сигнала также предлагается подавать в модель функции награды только вектор влияния.
Предсказание награды только по вектору влияния позволяет неявно интегрировать в него информацию об отношении манипулятора и робота.
Без этого изменения вектор влияния может испытывать проблемы с тренировкой, поскольку единственный функционал, предоставляющий прямой обучающий сигнал для него - это дивергенция Кульбака-Лейблера между генеративным и апостериорным распределениями $u_t$.

Уместность введения подобного генеративного распределения полностью зависит от среды, в которой находится агент.
При наличии возможности в течение одной задачи получать объектные наблюдения, также возможные для получения в других задачах, данный подход теряет свои преимущества, поскольку алгоритм больше не может определить примерное состояние объекта по информации о решаемой им задаче и состоянию манипулятора.

\subsubsection{Многозадачный вектор влияния}
Вместо того, чтобы пытаться придать вектору влияния $u_t$ значение влияния манипулятора на объект в конкретном состоянии, можно рассмотреть вектор $u_t$ как сжатое представление состояния $s^r_t$, содержащее информацию о потенциальных взаимодействиях на объекты в любых состояниях.
Этого можно достигнуть, отказавшись от внедрения контекста задачи $c$ в генеративное распределение для $u_t$.
Тогда как для модели вывода \ref{eq:cema-td} выглядит невозможным отказаться от контекста задачи, в \ref{eq:cema-infl} это выглядит реализуемым, поскольку генеративное и апостериорное распределение отличаются не настолько сильно.
Пытаясь совместить генеративное и апостериорное распределение, мы пытаемся подстроиться к нашему набору задач, выучивая такое представление вектора влияния, которое было бы полезно вне зависимости от конкретной задачи.
Однако эта модель испытывает больше проблем с тренировкой, поскольку предсказать награду по не зависящему от задачи или положения объекта вектору влияния $u_t$ не представляется возможным.

\subsection{Архитектуры моделей}
Тогда как апостериорное распределение объектной части рассматривается в виде $q(s^o_t \mid s^o_{t-1}, o^o_t)$, такой вид распределения может проявлять себя не лучшим образом во время обучения алгоритма.
В отличие от манипулятора, случайные действия, производимые агентом в начале обучения, могут не затрагивать объект, тем самым не меняя его состояние в течение эпизода.
Это может крайне негативно сказываться на обучении рекуррентной нейронной сети, подстраивающейся под предсказание одного и того же состояния.
Для преодоления этого эффекта и одновременно улучшения его обобщающей способности предлагается упростить апостериорное распределение, приведя его к виду $q(s^o_t \mid s^o_{t-1})$.
Таким образом обеспечивается более четкий обучающий сигнал при изменении изображения, подающегося на вход.

Также, благодаря разделению скрытой переменной, появляется возможность разделить признаки, подающиеся в актора, критика и модель функции награды для предоставления более четкого сигнала при обучении тех или иных компонент алгоритма. В частности, вектор влияния лучше всего обучается при подаче в него только $u_t$, либо $u_t$ и $s^o_t$.

\begin{figure}[t]%{r}{0.35\textwidth}
    \centering
    % \vspace{-80pt}
    \scalebox{0.75}{
        \input{schemes/archs/cema-direct}
    }
    \scalebox{0.75}{
        \input{schemes/archs/cema-td}
    }
    \scalebox{0.75}{
        \input{schemes/archs/cema-influence}
    }
    \scalebox{0.75}{
        \input{schemes/archs/cema-generalize}
    }
    \caption{Предлагаемые архитектуры}
    \label{fig:archs}
\end{figure}

Проверенные в ходе исследования модели алгоритмов изображены графически на рис. \ref{fig:archs}.
Для удобства описания, алгоритмы, использующие факторизованные модели мира, приведены под названием CEMA (Cause-Effect Modeling Agent).
Таким образом, наиболее уместными выглядят следующие модели:
\begin{itemize}
    \item \textbf{CEMA-Direct}: Модель мира прямого влияния \ref{eq:instant}, $\pi(a_t \mid s^r_t, s^o_t, c)$, $\nu(s^r_t, s^o_t)$, $r(s^o_t)$
    \item \textbf{CEMA-TopDown}: Модель мира косвенного влияния \ref{eq:cema-td}, однозадачный вектор влияния, $\pi(a_t \mid s^r_t, s^o_t, u_t)$, $\nu(s^r_t, s^o_t, u_t)$, $r(u_t)$
    \item \textbf{CEMA-Influence}: Модель мира косвенного влияния \ref{eq:cema-infl}, однозадачный вектор влияния, $\pi(a_t \mid s^r_t, s^o_t, u_t)$, $\nu(s^r_t, s^o_t, u_t)$, $r(u_t)$
    \item \textbf{CEMA-Generalize}: Модель мира косвенного влияния \ref{eq:cema-infl}, многозадачный вектор влияния, $\pi(a_t \mid s^r_t, s^o_t, u_t, c)$, $\nu(s^r_t, s^o_t)$, $r(s^o_t, u_t)$ 
\end{itemize}


