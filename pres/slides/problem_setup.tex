\begin{frame}
\frametitle{Постановка задачи}
\begin{itemize}
    \item Среда: Марковский Процесс Принятия Решений $\left<S, A, R, p, \gamma\right>$
    \begin{itemize}
        \item $S$ -- пространство состояний
        \item $A$ -- пространство действий
        \item $p(s'\mid s, a)$ -- функция перехода состояний среды
        \item $R(s, s', a)$ -- функция вознаграждения среды
    \end{itemize}
    \item Задача: найти стратегию $\pi(a\mid s)$ максимизирующую ожидаемую отдачу $J(\pi, p, R) = \mathbb{E}_{p, \pi} \sum_t \gamma^t R_t$
    \item Модельное обучение с подкреплением: аппроксимировать функцию перехода состояний среды и функцию награды ``моделью мира'' $\hat{p}_{\theta}(s'\mid s, a)$, $\hat{R}_{\theta}(s, s', a)$ с которой агент может взаимодействовать с целью максимизации $J(\pi, \hat{p}_{\theta}, \hat{R}_{\theta})$. Модель мира обучается на буфере опыта, полученного из взаимодействия агента со средой.
    \item Обобщение между задачами: пусть дано распределение задач (каждая из которых является МППР) $p_{\text{train}}(\tau)$. Обучая модельного агента на задачах $\tau \in p_{\text{train}}(\tau)$, необходимо максимизировать ожидаемую отдачу на задачах $\tau \in p_{\text{test}}(\tau)$.
\end{itemize}
\end{frame}